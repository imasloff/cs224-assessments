\titledquestion{Machine Learning \& Neural Networks}[8] 
\begin{parts}

    
    \part[4] Adam Optimizer\newline
        Recall the standard Stochastic Gradient Descent update rule:
        \alns{
            	\btheta_{t+1} &\gets \btheta_t - \alpha \nabla_{\btheta_t} J_{\text{minibatch}}(\btheta_t)
        }
        where $t+1$ is the current timestep, $\btheta$ is a vector containing all of the model parameters, ($\btheta_t$ is the model parameter at time step $t$, and $\btheta_{t+1}$ is the model parameter at time step $t+1$), $J$ is the loss function, $\nabla_{\btheta} J_{\text{minibatch}}(\btheta)$ is the gradient of the loss function with respect to the parameters on a minibatch of data, and $\alpha$ is the learning rate.
        Adam Optimization\footnote{Kingma and Ba, 2015, \url{https://arxiv.org/pdf/1412.6980.pdf}} uses a more sophisticated update rule with two additional steps.\footnote{The actual Adam update uses a few additional tricks that are less important, but we won't worry about them here. If you want to learn more about it, you can take a look at: \url{http://cs231n.github.io/neural-networks-3/\#sgd}}
            
        \begin{subparts}

            \subpart[2]First, Adam uses a trick called {\it momentum} by keeping track of $\bm$, a rolling average of the gradients:
                \alns{
                	\bm_{t+1} &\gets \beta_1\bm_{t} + (1 - \beta_1)\nabla_{\btheta_t} J_{\text{minibatch}}(\btheta_t) \\
                	\btheta_{t+1} &\gets \btheta_t - \alpha \bm_{t+1}
                }
                where $\beta_1$ is a hyperparameter between 0 and 1 (often set to  0.9). Briefly explain in 2--4 sentences (you don't need to prove mathematically, just give an intuition) how using $\bm$ stops the updates from varying as much and why this low variance may be helpful to learning, overall.\newline

                {\color{red} \textbf{Solution:} \newline
                    The accumulated past gradients, represented by the momentum term $\bm$, are used to smooth the updates by providing a weighted average of the previous gradients. Each gradient update is influenced not only by the current gradient but also by the historical trend of gradients. As a result, the updates become more stable and less sensitive to sudden changes or noise in individual gradients. By incorporating information from past gradients, the optimizer can make more informed decisions about the direction of updates, leading to smoother trajectories in the optimization process and ultimately facilitating convergence towards the optimal solution. \newline
                }
                
            \subpart[2] Adam extends the idea of {\it momentum} with the trick of {\it adaptive learning rates} by keeping track of  $\bv$, a rolling average of the magnitudes of the gradients:
                \alns{
                	\bm_{t+1} &\gets \beta_1\bm_{t} + (1 - \beta_1)\nabla_{\btheta_t} J_{\text{minibatch}}(\btheta_t) \\
                	\bv_{t+1} &\gets \beta_2\bv_{t} + (1 - \beta_2) (\nabla_{\btheta_t} J_{\text{minibatch}}(\btheta_t) \odot \nabla_{\btheta_t} J_{\text{minibatch}}(\btheta_t)) \\
                	\btheta_{t+1} &\gets \btheta_t - \alpha \bm_{t+1} / \sqrt{\bv_{t+1}}
                }
                where $\odot$ and $/$ denote elementwise multiplication and division (so $\bz \odot \bz$ is elementwise squaring) and $\beta_2$ is a hyperparameter between 0 and 1 (often set to  0.99). Since Adam divides the update by $\sqrt{\bv}$, which of the model parameters will get larger updates?  Why might this help with learning? \newline

                {\color{red} \textbf{Solution:} \newline
                    Dividing the momentum term by the square root of the magnitude of the squared gradients (effectively scales down the updates for parameters that have large gradients and scales up the updates for parameters with small gradients. Consequently, parameters associated with larger gradients receive smaller updates, while those with smaller gradients receive larger updates. This adaptive adjustment of learning rates helps prevent the optimizer from overshooting or getting stuck in steep regions of the loss landscape. \newline
                }
                
                \end{subparts}
        
        
            \part[4] 
            Dropout\footnote{Srivastava et al., 2014, \url{https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf}} is a regularization technique. During training, dropout randomly sets units in the hidden layer $\bh$ to zero with probability $p_{\text{drop}}$ (dropping different units each minibatch), and then multiplies $\bh$ by a constant $\gamma$. We can write this as:
                \alns{
                	\bh_{\text{drop}} = \gamma \bd \odot \bh
                }
                where $\bd \in \{0, 1\}^{D_h}$ ($D_h$ is the size of $\bh$)
                is a mask vector where each entry is 0 with probability $p_{\text{drop}}$ and 1 with probability $(1 - p_{\text{drop}})$. $\gamma$ is chosen such that the expected value of $\bh_{\text{drop}}$ is $\bh$:
                \alns{
                	\mathbb{E}_{p_{\text{drop}}}[\bh_\text{drop}]_i = h_i \text{\phantom{aaaa}}
                }
                for all $i \in \{1,\dots,D_h\}$. 
            \begin{subparts}
            \subpart[2]
                What must $\gamma$ equal in terms of $p_{\text{drop}}$? Briefly justify your answer or show your math derivation using the equations given above. \newline

                {\color{red} \textbf{Solution:} \newline
                    The scaling factor $\gamma$ in dropout is crucial for maintaining the expected value of the hidden units during training. Given that each unit has a probability of being dropped out, $\gamma$ needs to compensate for the retained units to ensure that the expected value of the activations remains unchanged. Mathematically, $\gamma = \frac{1}{1-p_{drop}}$, which effectively scales up the activations of the retained units. This scaling ensures that the expected activations match those of the original network, promoting efficient learning without introducing biases due to dropout. \newline
                }
            
          \subpart[2] Why should dropout be applied during training? Why should dropout \textbf{NOT} be applied during evaluation? \textbf{Hint:} it may help to look at the dropout paper linked. \newline

            {\color{red} \textbf{Solution:} \newline
                During training, dropout is applied to prevent overfitting by introducing noise and promoting the learning of more robust features. By randomly dropping units, dropout encourages neurons to learn more diverse representations, preventing them from relying too heavily on any specific subset of features. This regularization technique helps generalize better to unseen data by discouraging complex co-adaptations among neurons. However, during evaluation, dropout should be turned off because we want the model to utilize all learned features for making predictions without introducing unnecessary randomness. Instead, the activations are scaled by $\gamma$ to maintain the expected activations learned during training. This ensures consistent behavior between training and evaluation phases, allowing the model to produce reliable predictions without dropout-induced variations.
            }
         
        \end{subparts}


\end{parts}
